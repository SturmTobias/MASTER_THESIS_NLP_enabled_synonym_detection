{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f1e799d-e034-4ed2-a08f-9aa660272417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses, util, LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryClassificationEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "import logging\n",
    "import csv\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "304e74e6-a4d6-4be6-9e14-d64da4fc8e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:27:24 - Dataset not found. Download\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695e53f895964452a25ca934b103eb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/93.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "#You can specify any huggingface/transformers pre-trained model here, for example, bert-base-uncased, roberta-base, xlm-roberta-base\n",
    "model_name = 'bert-base-uncased'\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "max_seq_length = 128\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "###### Read Datasets ######\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "qqp_dataset_path = 'quora-IR-dataset'\n",
    "\n",
    "\n",
    "# Check if the STSb dataset exsist. If not, download and extract it\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "\n",
    "# Check if the QQP dataset exists. If not, download and extract\n",
    "if not os.path.exists(qqp_dataset_path):\n",
    "    logging.info(\"Dataset not found. Download\")\n",
    "    zip_save_path = 'quora-IR-dataset.zip'\n",
    "    util.http_get(url='https://sbert.net/datasets/quora-IR-dataset.zip', path=zip_save_path)\n",
    "    with ZipFile(zip_save_path, 'r') as zipIn:\n",
    "        zipIn.extractall(qqp_dataset_path)\n",
    "\n",
    "\n",
    "cross_encoder_path = 'output/cross-encoder/stsb_indomain_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "bi_encoder_path = 'output/bi-encoder/qqp_cross_domain_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d99a3-5230-4e0c-b6dd-74bf2e58caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cross-encoder (simpletransformers) ######\n",
    "\n",
    "#logging.info(\"Loading cross-encoder model: {}\".format(model_name))\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for cross-encoder model\n",
    "#cross_encoder = CrossEncoder(model_name, num_labels=1)\n",
    "\n",
    "###### Bi-encoder (sentence-transformers) ######\n",
    "\n",
    "logging.info(\"Loading bi-encoder model: {}\".format(model_name))\n",
    "\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725aa55f-f643-4b3b-b166-54f2906160c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#\n",
    "# Train bi-encoder (BERT) model with previously scored activity label pairs (this bi-encoder is the the final language model which can than be used as the basis for synonym detection applications)\n",
    "#\n",
    "###########################################################################\n",
    "\n",
    "logging.info(\"Train bi-encoder: {} over labeled QQP (target dataset)\".format(model_name))\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Loading BERT labeled QQP dataset\")\n",
    "to_be_scored = list(InputExample(texts=[data[0], data[1]], label=score) for (data, score) in zip(silver_data, binary_silver_scores))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(to_be_scored, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(bi_encoder)\n",
    "\n",
    "###### Classification ######\n",
    "# Given (activity label 1, activity label 2), is this a duplicate or not?\n",
    "# The evaluator will compute the embeddings for both labels and then compute\n",
    "# a cosine similarity. If the similarity is above a threshold, we have a duplicate.\n",
    "logging.info(\"Read activity label dataset\")\n",
    "\n",
    "dev_sentences1 = []\n",
    "dev_sentences2 = []\n",
    "dev_labels = []\n",
    "\n",
    "with open(os.path.join(to_be_scored, \"classification/dev_pairs.tsv\"), encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        dev_sentences1.append(row['question1'])\n",
    "        dev_sentences2.append(row['question2'])\n",
    "        dev_labels.append(int(row['is_duplicate']))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(dev_sentences1, dev_sentences2, dev_labels)\n",
    "\n",
    "# Configure the training.\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the bi-encoder model\n",
    "bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=bi_encoder_path\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98915057-c52f-4e52-9169-62b1588959d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:21:26 - Load pretrained SentenceTransformer: output/bi-encoder/qqp_cross_domain_bert-base-uncased-2022-03-17_15-25-31\n",
      "2022-03-17 16:21:27 - Use pytorch device: cuda\n",
      "2022-03-17 16:21:27 - Read QQP test dataset\n",
      "2022-03-17 16:21:27 - Binary Accuracy Evaluation of the model on  dataset:\n",
      "2022-03-17 16:21:27 - Accuracy with Cosine-Similarity:           78.08\t(Threshold: 0.5953)\n",
      "2022-03-17 16:21:27 - F1 with Cosine-Similarity:                 79.12\t(Threshold: 0.4262)\n",
      "2022-03-17 16:21:27 - Precision with Cosine-Similarity:          66.85\n",
      "2022-03-17 16:21:27 - Recall with Cosine-Similarity:             96.90\n",
      "2022-03-17 16:21:27 - Average Precision with Cosine-Similarity:  84.36\n",
      "\n",
      "2022-03-17 16:21:27 - Accuracy with Manhatten-Distance:           80.25\t(Threshold: 287.1740)\n",
      "2022-03-17 16:21:27 - F1 with Manhatten-Distance:                 80.55\t(Threshold: 297.9510)\n",
      "2022-03-17 16:21:27 - Precision with Manhatten-Distance:          72.03\n",
      "2022-03-17 16:21:27 - Recall with Manhatten-Distance:             91.37\n",
      "2022-03-17 16:21:27 - Average Precision with Manhatten-Distance:  85.65\n",
      "\n",
      "2022-03-17 16:21:27 - Accuracy with Euclidean-Distance:           80.54\t(Threshold: 12.8887)\n",
      "2022-03-17 16:21:27 - F1 with Euclidean-Distance:                 80.65\t(Threshold: 13.8874)\n",
      "2022-03-17 16:21:27 - Precision with Euclidean-Distance:          70.51\n",
      "2022-03-17 16:21:27 - Recall with Euclidean-Distance:             94.18\n",
      "2022-03-17 16:21:27 - Average Precision with Euclidean-Distance:  85.80\n",
      "\n",
      "2022-03-17 16:21:27 - Accuracy with Dot-Product:           75.37\t(Threshold: 80.6642)\n",
      "2022-03-17 16:21:27 - F1 with Dot-Product:                 78.38\t(Threshold: 71.8397)\n",
      "2022-03-17 16:21:27 - Precision with Dot-Product:          65.46\n",
      "2022-03-17 16:21:27 - Recall with Dot-Product:             97.65\n",
      "2022-03-17 16:21:27 - Average Precision with Dot-Product:  81.74\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8579594191354043"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################\n",
    "#\n",
    "# Evaluate performance on test sample \n",
    "# (using 15-fold cross-validation to queck performance on each of the available domains) \n",
    "# \n",
    "###############################################################\n",
    "\n",
    "path = ''\n",
    "\n",
    "# Loading the augmented sbert model \n",
    "bi_encoder = SentenceTransformer(bi_encoder_path)\n",
    "\n",
    "logging.info(\"Read test dataset\")\n",
    "test_sentences1 = []\n",
    "test_sentences2 = []\n",
    "test_labels = []\n",
    "\n",
    "with open(path, encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=';', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        test_sentences1.append(row['activity1'])\n",
    "        test_sentences2.append(row['activity2'])\n",
    "        test_labels.append(int(row['annotation']))\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(test_sentences1, test_sentences2, test_labels)\n",
    "bi_encoder.evaluate(evaluator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
