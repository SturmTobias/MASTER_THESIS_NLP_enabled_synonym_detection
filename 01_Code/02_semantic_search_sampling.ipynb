{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5683d-b586-4db6-9d8b-3028c6023adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on 6250 distinct labels (625 labels and 10 synonyms per label retrieved and consolidated) we would get approx. 1,9 Mio. possible activity label pair combinations. \n",
    "# Therefore strucuture approach 'semantic search pair sampling' based on idea from Thakur et al. 2021 to retrieve high scored synonyms to reduce computational overhead.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b47a45e3-537a-4226-8ff5-cfaf87b55ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses, util\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import csv\n",
    "import torch\n",
    "import tqdm\n",
    "import sys\n",
    "import math\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "48f1f22e-4506-4d22-badd-948e84d5600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source_path = '' #specify source path of csv file\n",
    "output_path = '' # specify path where pairs should be saved\n",
    "csv = pd.read_csv(source_path, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "001eb78f-5de7-4688-981d-523a4bfa96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(csv['augmented_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1cf92b52-b828-463b-bb26-16515d674de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Top k similar sentences to be retrieved ####\n",
    "#### Larger the k, bigger the silver dataset ####\n",
    "top_k = 6\n",
    "\n",
    "silver_data = []\n",
    "\n",
    "sent2idx = {sentence: idx for idx, sentence in enumerate(sentences)} # storing id and sentence in dictionary\n",
    "#duplicates = set((sent2idx[data.texts[0]], sent2idx[data.texts[1]]) for data in gold_samples) # not to include gold pairs of sentences again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6c1c31b4-737c-4550-b17d-aa955ac5e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained version of BERT 'all-mpnet-base-v2'\n",
    "\n",
    "semantic_model_name = 'all-mpnet-base-v2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "99bf9098-d0b6-4fab-a016-dd7c62d8dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all unique sentences present in the training dataset using semantic model\n",
    "logging.info(\"Encoding unique sentences with semantic search model: {}\".format(semantic_search_model))\n",
    "embeddings = semantic_search_model.encode(sentences, batch_size=batch_size, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3914d476-c9ca-4bee-a780-f2ba88c9cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving top-k sentences given a sentence from the dataset\n",
    "#progress = tqdm.tqdm(unit=\"docs\", total=len(sent2idx))\n",
    "for idx in range(len(sentences)):\n",
    "    sentence_embedding = embeddings[idx]\n",
    "    cos_scores = util.cos_sim(sentence_embedding, embeddings)[0]\n",
    "    cos_scores = cos_scores.cpu()\n",
    "    #progress.update(1)\n",
    "\n",
    "    #torch.topk to find the highest 10 scores\n",
    "    top_results = torch.topk(cos_scores, k=top_k+1)\n",
    "    \n",
    "    for score, iid in zip(top_results[0], top_results[1]):\n",
    "        if iid != idx: #and (iid, idx) not in duplicates:\n",
    "            silver_data.append((sentences[idx], sentences[iid]))\n",
    "            #duplicates.add((idx,iid))\n",
    "\n",
    "#progress.reset()\n",
    "#progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1ada720f-4a4a-4bc2-90c3-ee9b89d9b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the zipped list into pandas dataframe and dropping out duplicates\n",
    "# since the augmenting enginge generates the same synonyms sometimes\n",
    "df_silver = pd.DataFrame(silver_data, columns = ['label1','label2'])\n",
    "df_silver_without_duplicates = df_silver.drop_duplicates()\n",
    "df_silver_without_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "81479ce2-1b43-45e2-a92c-0a7568fd78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataframe without duplicates as .csv file\n",
    "\n",
    "header = [\"label1\", \"label2\"]\n",
    "#df.to_csv('output.csv', columns = header)\n",
    "\n",
    "df_silver_without_duplicates.to_csv(output_path, sep=';', columns = header, header=True, index=False)# sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
