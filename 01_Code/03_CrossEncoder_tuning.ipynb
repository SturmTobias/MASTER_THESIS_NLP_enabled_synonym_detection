{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37a646-8540-4774-8e2d-9371b3ca960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Trained Cross-Encoder model now is trained with STSbenchmark dataset (labeled dataset). \n",
    "# The model is afterwards used to score activirty label pairs retrieved in 02 (semantic search pair sampling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1e799d-e034-4ed2-a08f-9aa660272417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses, util, LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryClassificationEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "import logging\n",
    "import csv\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import gzip\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304e74e6-a4d6-4be6-9e14-d64da4fc8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# specify hyperparameters\n",
    "model_name = 'bert-base-uncased'\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "max_seq_length = 128\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67500d4c-87ec-473b-8486-2d33d9ca3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read Datasets ######\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "\n",
    "# Check if the STSb dataset exsist. If not, download and extract it\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "# Specify output path where fine-tuned cross-encoder should be saved\n",
    "cross_encoder_path = 'output/cross-encoder/stsb_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d99a3-5230-4e0c-b6dd-74bf2e58caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Configuration of Cross-encoder ######\n",
    "###### When executing this part some red highlighted messages appear, these can be ignored\n",
    "\n",
    "logging.info(\"Loading cross-encoder model: {}\".format(model_name))\n",
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for cross-encoder model\n",
    "cross_encoder = CrossEncoder(model_name, num_labels=1)\n",
    "\n",
    "# Use Huggingface/transformers model (BERT) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d770fc-c1ab-4656-9805-f22432f4f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#\n",
    "# Train cross-encoder model with STSbenchmark \n",
    "# (this training is only conducted one time to automatically label the activity labels dataset which is than used to train the final bi-encoder)\n",
    "#\n",
    "#####################################################\n",
    "\n",
    "logging.info(\"Step 1: Train cross-encoder: {} with STSbenchmark (source dataset)\".format(model_name))\n",
    "\n",
    "gold_samples = [] #training samples\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "# splitting data into training and test sets\n",
    "\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1\n",
    "\n",
    "        if row['split'] == 'dev':\n",
    "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "        elif row['split'] == 'test':\n",
    "            test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "        else:\n",
    "            #As we want to get symmetric scores, i.e. CrossEncoder(A,B) = CrossEncoder(B,A), we pass both combinations to the train set\n",
    "            gold_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "            gold_samples.append(InputExample(texts=[row['sentence2'], row['sentence1']], label=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d1a27-bb29-4e03-a7bd-5f596d22698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap gold_samples (which is a List[InputExample]) into a pytorch DataLoader\n",
    "train_dataloader = DataLoader(gold_samples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# We add an evaluator, which evaluates the performance during training\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the cross-encoder model\n",
    "cross_encoder.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=cross_encoder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
